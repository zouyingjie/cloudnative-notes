# 分布式锁那些事

## 具体实现

### 1. Redis 方案

关于分布式锁介绍最多的应该就是基于 Redis 的实现了，我们先来看下。

在单节点下的 Redis 获取锁的方式是通过如下命令：

```sh
SET resource_name my_random_value NX PX 30000
```

- NX（not exist） 表示 只有在 key 不存在时才能设置成功，避免重复加锁。
- PX 表示过期时间为 30s，保证即使持有锁的客户端挂掉也能及时的释放锁。
- 值需要设置为一个全局唯一的随机数，在删除锁时会用到。

删除锁则可以通过 LUA 脚本实现

```lua
if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end
```

可以看到 Redis 提供的命令能满足分布式锁的两个要求：

- 互斥：通过 NX 保证 key 的唯一，从而保证互斥。
- PX 提供锁的过期时间，避免死锁

与此同时为了避免持有锁的客户端崩溃恢复后将锁误删，使用了随机数进行删除确认。比如客户端 A 获取了锁，但因为 GC 或者其他原因长时间阻塞，锁超时后被释放掉由客户端 B 获得了锁，此时 A 恢复后如果直接删除 key 会导致 B 的锁被误删，通过全局唯一随机数的方式保证了客户端只能删除自己获取到的锁。

Redis 单节点方案满足了互斥性和活性，但存在单点故障问题，不满足高可用性。与此同时，Redis 不是强一致性的存储服务，其主副本同步是异步的，没办法通过增加副本的方式满足上述需求，为此有人提出了 RedLock 算法。


![](https://pub-08b57ed9c8ce4fadab4077a9d577e857.r2.dev/distributed-lock-01.png)

#### RedLock 算法

RedLock 算法要求使用多个 Redis 节点（通常是 5 个）实现高可用的分布式锁服务。获取锁的步骤如下：

- 客户端获取当前的毫秒时间戳
- 使用相同的 key、value 按顺序从从每个节点获取锁。这一步需要设置一个相对于锁的过期时间较小的超时时间，比如锁的过期时间为 10s，我们可以设置超时时间为 5 ~ 50ms，避免客户端一直在等待获取锁。
- 只有在多数节点获取锁成功（比如 3 个节点加锁成功）以及总耗时小于锁的初始有效期时，才可以认为获取锁成功。
- 如果加锁成功，为了保证所有节点的锁能在同一时间失效，锁的过期时间为 **初始过期时间减去加锁耗时**。比如加第一个节点的锁时超时时间为 10s，加完耗时 5ms，那么第二个锁的超时时间应该是 10s-5ms = 9.995s，以此类推。
- 如果获取锁失败，会将所有实例的锁释放掉。

不知道大家是否有和笔者一样的感受，在学习这个算法时感觉非常绕。首先，RedLock 算法的实现依赖于至少 5 个独立的 Redis 节点，运维相对麻烦。其次也是最重要的是，RedLock 算法本身是依赖于系统时钟的，而在分布式系统中，系统时钟和网络是最不可靠的两个因素，它并不能提供足够的安全性。

我们假设有 A、B、C、D、E 共 5 个 Redis 节点，此时有客户端 A 和 B 来获取锁，首先 A 从 A、B、C 获取到了锁，然后 C 节点的系统时钟向前发生了偏移，这会导致 C 节点的锁提前过期，此时客户 B 来获取锁会在 C、D、E
节点获取成功，此时 A 和 B 都会持有锁。

在笔者看来， Redis 本身并不是一个强一致性的存储服务，更多的是在缓存等数据安全性要求不高的场景中使用，本身不适合分布式锁这种对数据的一致性、服务的高可用性有极高要求的场景。RedLock 算法本身是为了基于 Redis 实现一个高可用分布式锁服务强行设计出的一个解决方案，因此整个方案显得非常不优雅。

对于 RedLock 算法，《数据密集型应用系统设计》的作者 Martin Kleppmann 在其文章中 [How to do distributed locking](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)，强烈推荐阅读。
虽然 RedLock 算法的作者进行了一些[反驳](https://antirez.com/news/101)，但也承认对于时钟偏移的情况 RedLock 算法是无法解决的。

Martin Kleppmann 提出，一个靠谱的分布式锁需要一个隔离令牌（fence token）来保证只有一个客户端持有锁。如图所示：

- 客户端 1 获取到锁执行任务，其 token 为 33
- 客户端 1 因为 GC 等原因暂停运行，最终分布式锁服务中的锁过期
- 客户端 2 获取到锁，此时其 token 应该递增，比如变为 34
- 客户端 1 恢复运行，此时还认为自己持有锁，继续执行读写任务
- 存储服务检查到客户端 1 的 token 为 33 已经是过时的，因此拒绝其读写任务，要求重新获取锁
  
![](https://pub-08b57ed9c8ce4fadab4077a9d577e857.r2.dev/fencing-tokens.png)

可以看到这本质上是一种类似乐观锁的机制，其要求生成锁的服务能够提供一个 token 令牌，同时读写数据时存储服务能够校验令牌。Zookeeper 的实现能符合这一要求，这也是 Martin Kleppmann  所推荐的实现方案。

### 2. Zookeeper 方案

Zookeeper 组织数据的方式类似于操作系统的文件系统，其存储数据的位置被 znode，类似于操作系统中的文件。

![](https://pub-08b57ed9c8ce4fadab4077a9d577e857.r2.dev/zookeeper-01.png)

Zookeeper 的节点有两类：

- **正常节点（Regular）**：正常存储数据的节点，需求通过客户端读写来增删节点。
- **临时节点（Ephemeral）**：临时节点，客户端会话断开后会自动删除。

另外，Zookeeper 提供了一个 ``sequential`` 标签，用于在创建节点时标识其是否是顺序节点，在顺序节点下，我们可以利用该特性实现锁。加锁过程和伪代码如下：

```c
// 加锁
n = create(l + “/lock-”, EPHEMERAL|SEQUENTIAL) 
C = getChildren(l, false) 
if n is lowest znode in C, exit 4 p = znode in C ordered just before n 5 if exists(p, true) wait for watch event 6 goto 2 Unlock 1 delete(n
```

对于访问相同资源的锁，我们使用同样的前缀去创建 **顺序临时节点**










### 3. Zookeeper 方案

### 4. Etcd 方案

